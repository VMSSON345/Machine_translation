# Machine_translation


# Há»‡ thá»‘ng Dá»‹ch mÃ¡y Anh-Viá»‡t sá»­ dá»¥ng Kiáº¿n trÃºc Transformer

Dá»± Ã¡n nÃ y trÃ¬nh bÃ y viá»‡c xÃ¢y dá»±ng vÃ  tá»‘i Æ°u hÃ³a má»™t há»‡ thá»‘ng dá»‹ch mÃ¡y tháº§n kinh (Neural Machine Translation - NMT) cho cáº·p ngÃ´n ngá»¯ Anh-Viá»‡t dá»±a trÃªn kiáº¿n trÃºc Transformer.

[](https://www.python.org/)
[](https://opensource.org/licenses/MIT)

-----

## ğŸ“œ Giá»›i thiá»‡u

Há»‡ thá»‘ng nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn mÃ´ hÃ¬nh Transformer, má»™t kiáº¿n trÃºc tiÃªn tiáº¿n trong lÄ©nh vá»±c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, Ä‘á»ƒ dá»‹ch tá»« tiáº¿ng Anh sang tiáº¿ng Viá»‡t. Dá»± Ã¡n khÃ´ng chá»‰ táº­p trung vÃ o viá»‡c Ä‘áº¡t Ä‘Æ°á»£c cháº¥t lÆ°á»£ng dá»‹ch thuáº­t cao mÃ  cÃ²n chÃº trá»ng Ä‘áº¿n cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh Ä‘á»ƒ cÃ³ thá»ƒ triá»ƒn khai trong cÃ¡c á»©ng dá»¥ng thá»±c táº¿.

CÃ¡c thÃ nh pháº§n chÃ­nh cá»§a dá»± Ã¡n bao gá»“m:

  * **Kiáº¿n trÃºc Transformer**: Táº­n dá»¥ng cÆ¡ cháº¿ tá»± chÃº Ã½ (self-attention) Ä‘á»ƒ xá»­ lÃ½ cÃ¡c phá»¥ thuá»™c xa trong cÃ¢u.
  * **Tokenization**: Sá»­ dá»¥ng SentencePiece Ä‘á»ƒ táº¡o ra cÃ¡c token á»Ÿ má»©c subword, giÃºp xá»­ lÃ½ tá»« vá»±ng lá»›n vÃ  cÃ¡c tá»« khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn.
  * **Tá»‘i Æ°u hÃ³a**: Ãp dá»¥ng cÃ¡c ká»¹ thuáº­t nhÆ° lÆ°á»£ng tá»­ hÃ³a Ä‘á»™ng (dynamic quantization) vÃ  xuáº¥t mÃ´ hÃ¬nh sang Ä‘á»‹nh dáº¡ng ONNX Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c vÃ  tÄƒng tÃ­nh tÆ°Æ¡ng thÃ­ch.

-----

## ğŸš€ TÃ­nh nÄƒng ná»•i báº­t

  * **Cháº¥t lÆ°á»£ng dá»‹ch thuáº­t cáº¡nh tranh**: Äáº¡t Ä‘iá»ƒm BLEU **24.30** vá»›i chiáº¿n lÆ°á»£c giáº£i mÃ£ Beam Search.
  * **MÃ´ hÃ¬nh hiá»‡u quáº£**: Giáº£m kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh khoáº£ng **74.64%** (tá»« 229.75 MB xuá»‘ng 58.25 MB) thÃ´ng qua lÆ°á»£ng tá»­ hÃ³a Ä‘á»™ng, giÃºp triá»ƒn khai dá»… dÃ ng hÆ¡n trÃªn cÃ¡c thiáº¿t bá»‹ cÃ³ tÃ i nguyÃªn háº¡n cháº¿.
  * **TÃ­nh di Ä‘á»™ng**: Há»— trá»£ xuáº¥t mÃ´ hÃ¬nh sang Ä‘á»‹nh dáº¡ng ONNX, táº¡o Ä‘iá»u kiá»‡n thuáº­n lá»£i cho viá»‡c triá»ƒn khai trÃªn nhiá»u ná»n táº£ng khÃ¡c nhau.

-----

## ğŸ› ï¸ Kiáº¿n trÃºc vÃ  PhÆ°Æ¡ng phÃ¡p

### **1. Dá»¯ liá»‡u vÃ  Tiá»n xá»­ lÃ½**

  * **Táº­p dá»¯ liá»‡u**: Sá»­ dá»¥ng bá»™ dá»¯ liá»‡u song ngá»¯ Anh-Viá»‡t IWSLT15 tá»« Hugging Face Datasets.
  * **Tiá»n xá»­ lÃ½**: Dá»¯ liá»‡u Ä‘Æ°á»£c lÃ m sáº¡ch báº±ng cÃ¡ch loáº¡i bá» khoáº£ng tráº¯ng thá»«a, chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng vÃ  chuáº©n hÃ³a Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n.

### **2. Tokenization**

  * ChÃºng tÃ´i sá»­ dá»¥ng **SentencePiece** Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c tokenizer riÃªng biá»‡t cho tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t.
  * KÃ­ch thÆ°á»›c tá»« vá»±ng lÃ  **30,000** cho tiáº¿ng Anh vÃ  **15,000** cho tiáº¿ng Viá»‡t.

### **3. Kiáº¿n trÃºc MÃ´ hÃ¬nh**

  * MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn kiáº¿n trÃºc Transformer tiÃªu chuáº©n, bao gá»“m má»™t bá»™ mÃ£ hÃ³a (encoder) vÃ  má»™t bá»™ giáº£i mÃ£ (decoder).
  * **Hyperparameters**:
      * Sá»‘ lá»›p Encoder & Decoder: 4
      * Sá»‘ lÆ°á»£ng attention heads: 4
      * KÃ­ch thÆ°á»›c embedding: 512
      * Tá»· lá»‡ Dropout: 0.25

### **4. Huáº¥n luyá»‡n**

  * **TrÃ¬nh tá»‘i Æ°u hÃ³a**: AdamW.
  * **HÃ m máº¥t mÃ¡t**: Cross-Entropy vá»›i ká»¹ thuáº­t lÃ m má»‹n nhÃ£n (label smoothing).
  * **Bá»™ Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c**: Cosine Annealing.
  * MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trong 30 epochs vá»›i cÆ¡ cháº¿ dá»«ng sá»›m (early stopping) Ä‘á»ƒ trÃ¡nh overfitting.

-----

## ğŸ“Š Káº¿t quáº£

### **Cháº¥t lÆ°á»£ng dá»‹ch thuáº­t**

Há»‡ thá»‘ng Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ báº±ng Ä‘iá»ƒm **BLEU** trÃªn táº­p validation.

| Chiáº¿n lÆ°á»£c giáº£i mÃ£ | Äiá»ƒm BLEU |
| :--- | :---: |
| Greedy Decoding | 23.53 |
| Beam Search (Beam Width = 4) | **24.30** |

Káº¿t quáº£ cho tháº¥y Beam Search mang láº¡i hiá»‡u suáº¥t dá»‹ch tá»‘t hÆ¡n so vá»›i Greedy Decoding.

### **Hiá»‡u quáº£ MÃ´ hÃ¬nh**

Ká»¹ thuáº­t lÆ°á»£ng tá»­ hÃ³a Ä‘á»™ng Ä‘Ã£ giáº£m Ä‘Ã¡ng ká»ƒ kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh mÃ  váº«n duy trÃ¬ cháº¥t lÆ°á»£ng dá»‹ch thuáº­t.

  * **KÃ­ch thÆ°á»›c mÃ´ hÃ¬nh gá»‘c**: 229.75 MB
  * **KÃ­ch thÆ°á»›c mÃ´ hÃ¬nh Ä‘Ã£ lÆ°á»£ng tá»­ hÃ³a**: 58.25 MB (Giáº£m **74.64%**)

-----

## âš™ï¸ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t vÃ  Sá»­ dá»¥ng

### **YÃªu cáº§u**

  * Python 3.8+
  * PyTorch
  * Hugging Face Datasets
  * SentencePiece
  * (CÃ¡c thÆ° viá»‡n khÃ¡c Ä‘Æ°á»£c liá»‡t kÃª trong `requirements.txt`)

### **CÃ¡c bÆ°á»›c cÃ i Ä‘áº·t**

1.  **Clone repository:**

    ```bash
    git clone https://github.com/VMSSON345/Machine_translation.git
    ``

2.  **CÃ i Ä‘áº·t cÃ¡c gÃ³i phá»¥ thuá»™c:**

    ```bash
    pip install -r requirements.txt
    ```

### **Sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘á»ƒ dá»‹ch**

Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng script Ä‘Æ°á»£c cung cáº¥p Ä‘á»ƒ dá»‹ch má»™t cÃ¢u tá»« tiáº¿ng Anh sang tiáº¿ng Viá»‡t.

```bash
python translate.py --text "Glad to see you here!" --model_path "path/to/your/best_model.pt"
```

-----

## ğŸ’¡ HÆ°á»›ng phÃ¡t triá»ƒn trong tÆ°Æ¡ng lai

  * **TÄƒng cÆ°á»ng dá»¯ liá»‡u**: Ãp dá»¥ng cÃ¡c ká»¹ thuáº­t nhÆ° back-translation Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ máº¡nh máº½ cá»§a mÃ´ hÃ¬nh.
  * **Tinh chá»‰nh trÃªn cÃ¡c táº­p dá»¯ liá»‡u chuyÃªn ngÃ nh**: TÃ¹y chá»‰nh mÃ´ hÃ¬nh cho cÃ¡c lÄ©nh vá»±c cá»¥ thá»ƒ nhÆ° y táº¿ hoáº·c cÃ´ng nghá»‡ Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c.
  * **ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t**: Äo lÆ°á»ng Ä‘á»™ trá»… vÃ  thÃ´ng lÆ°á»£ng cá»§a mÃ´ hÃ¬nh Ä‘Ã£ lÆ°á»£ng tá»­ hÃ³a trÃªn cÃ¡c thiáº¿t bá»‹ pháº§n cá»©ng khÃ¡c nhau.

-----

## ğŸ™ Lá»i cáº£m Æ¡n

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cÃ¡c nghiÃªn cá»©u vÃ  cÃ´ng cá»¥ mÃ£ nguá»“n má»Ÿ tuyá»‡t vá»i tá»« cá»™ng Ä‘á»“ng. ChÃºng tÃ´i xin chÃ¢n thÃ nh cáº£m Æ¡n cÃ¡c tÃ¡c giáº£ cá»§a nhá»¯ng cÃ´ng trÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch dáº«n.

-----

## ğŸ“„ Giáº¥y phÃ©p

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c cáº¥p phÃ©p theo Giáº¥y phÃ©p MIT. Xem chi tiáº¿t táº¡i file `LICENSE`.
